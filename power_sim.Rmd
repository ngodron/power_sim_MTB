---
title: "Power analysis and bacterial population sizes"
author: "Josselin Noirel, Antoine Bridier-Nahmias, Nicolas Godron"
date: "2024 -- `r Sys.Date()`"
output: html_document
---

```{r Setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, comment=NULL)
```

```{r Libraries, message=FALSE, warning=FALSE}
library('foreach')
library('doParallel')
library('tidyverse')
library('scales')

library('lemon')
knit_print.data.frame <- lemon_print

theme_set(theme_bw())
set.seed(123)

registerDoParallel(cores=16)
```

## Background

We establish a simple, additive genotype-phenotype relationship in a bacterial population based on polymorphic genes.  The genes are assumed to be in linkage equilibrium and there is no genetic interaction in our model. Given a certain population size, we simulate data to estimate how many true associations get detected.

## Parametrisation of the genotype-phenotype relationship

Let's assume we have $n = 15$ independent causal genes (as there are elements towards polygenic inheritance) among $N = 4000$ (order of magnitude of $\ Mycobacterium\ tuberculosis $ genes) bi-allelic genes.

```{r Parameters}
n = 15             # Number of independent (i.e. in linkage equilibrium) causal loci
N = 4000           # Number of loci to be tested; N >= n (N - n non causal)
```

We'll assume a simple, additive genotype-phenotype relationship in a bacterium.  In this context, the genetic architecture is defined by the allele frequency and the penetrance of each polymorphic gene.  We assume that there is no linkage  We denote each causal gene has a MAF $f_i$ and an effect size $\beta_i$.

A simple genetic architecture could assume identical MAFs and identical $\beta_i$'s.

```{r, eval=FALSE}
f = rep(.25, n)    # Minor allele frequency (let's assume constant)
beta = rep(1, n)   # Identical, arbitrary effect size for all genes
```

Here, we will use a genetic architecture that assumes an equally distributed mixture of rare, uncommon and common variants $f_i \in \{1\%,5\%,20\%\}$, each with a spectrum of effect sizes: $\beta_i \in  [\![ 1, 2, .., 10]\!]$.

```{r}
f_vector = c(0.01, 0.05, 0.20)
f = rep(f_vector,
        each = n / length(f_vector)) # Rare, uncommon and common variants
beta = rep(1:(n / length(f_vector)),
              times = length(f_vector))        # Mixture of effect sizes
```

Note that the magnitude of the effect sizes $\beta_i$'s is determined up to a constant multiplier (so that `beta = rep(1, n)` and `beta = rep(10, n)` are equivalent).

We model a quantitative phenotype $Y$ through an additive relationship
\[ Y = \sum \beta_i d_i + \epsilon \]
where $d_i$ (0 or 1) denotes the $i$-th genotype (presence/absence of gene) $\epsilon$ is a normal distribution with a mean of zero and variance of $\sigma^2$; it captures the part of the phenotype that is determined by unaccounted-for variables (be they genetic, environmental or otherwise) as well as the part of the phenotype that is, for all intents and purposes, purely stochastic.
\[\bar{Y} = \sum \beta_i f_i \quad
  V(Y)    = \sum \beta_i f_i (1 - f_i) + \sigma^2 \]

Narrow-sense heritability is given by \[ h^2 = \frac{\sum \beta_i f_i (1 - f_i)}{V(Y)} \]

For this phenotype, we assume a fair amount of heritability $h^2 = 30\%$, to account for host factors such as immune competency (or lack thereof) making up part of the effect.
Another assumption is the confounding contribution of all resistances, which here will be set at $10$%, so $h^2\_res = 20%$.
This heritability would gain from managing resistance-induced bias, such as designing a resistance covariate. Principal Component Analysis results (data not shown) suggest building such covariate is doable.

```{r}
h2 = 0.30
h2_res = 0.20
```

The amount of noise, $\sigma^2$, can be determined based on the assumed value for $h^2$:
\[ \sigma^2 = \frac{1 - h^2}{h^2} \Bigl( \sum \beta_i f_i (1 - f_i) \Bigl) \]

```{r}
vg = sum(beta^2 * f * (1 - f)) # Genetic variation
s2 = (1 - h2)/h2 * vg          # sigma^2
vt = vg + s2                   # Total phenotypic variation
sigma  = sqrt(s2)              # sigma

s2_res = (1- h2_res)/h2_res * vg
vt_res = vg + s2_res
sigma_res  = sqrt(s2_res)
```

Other genetic architectures could be tested:

* 10%, 15%, 40%, 60% heritability
* .02, .1, .3, .4  MAF
* 4, 8, 16, 32, 64 independent causal loci
* Include the alpha model for effect size https://www.nature.com/articles/s41467-019-08424-6

We assume $K = 407$ for the set of genomes with no resistance and $K\_res = 1000$ for those with or without resistance.
A majority of the strains in $K\_res$ but not in $K$ would be publicly available genomes from published studies, some of which have already been collected and passed our genomic quality control pipeline. The rest would be newly included samples from patients suffering from meningitis, sequenced by the CNR.

```{r}
K = 407
K_res = 1000
```

Detection is based on a type~I error rate of $\alpha = 0.05\ (5\%)$ or False Discovery Rate $FDR = 50\%$.

```{r}
alpha = .05
fdr_thr = .5
```

## Simulations

We perform $R = 10,000$ replicates:

```{r}
R = 10000       # Number of replicates
```

Each replicate will build a matrix of genotypes and a phenotype including noise.  Genetic associations are sought using a simple $t$ test; the $p$ value is recorded.

For each simulation, we report:

* How many true hits are retrieved based on a Bonferroni correction
* How many      hits are reported using the FDR threshold and
* How many true hits are reported using the FDR threshold
* The minimal and the maximal $p$ values

```{r}
p <- numeric(n)  # Vector of n p-values

sim <- foreach(r = 1:R, .combine = rbind) %dopar%
  {
    x = t(matrix(rbinom(n * K, size=1, prob=f), nrow=n))
    yg = colSums(beta * t(x))
    noise = rnorm(K, mean=0, sd=sigma)
    y = yg + noise

    for (j in 1:n) {
        a = y[x[, j] == 0]
        b = y[x[, j] == 1]
        if (length(a) >=2 && length(b) >= 2) {
            test = t.test(a, b)
            p[j] = test$p.value
        } else {
            p[j] = 1
        }
    
        fdr = p.adjust(c(p, runif(N - n)), method='fdr')
        
        fdr_true_detected = sum(which(fdr < fdr_thr) <= n)
        
        fdr_detected = sum(fdr < fdr_thr)
    }
    
    c(minp=min(p),
      maxp=max(p),
      bonf_true=sum(p < alpha/N),
      fdr_detect=fdr_detected,
      fdr_true=fdr_true_detected,
      h2=var(yg)/var(y))
  }

# Same but with resistance dataset
sim_res <- foreach(r = 1:R, .combine = rbind) %dopar%
  {
    x = t(matrix(rbinom(n * K_res, size=1, prob=f), nrow=n))
    yg = colSums(beta * t(x))
    noise = rnorm(K_res, mean=0, sd=sigma_res)
    y = yg + noise

    for (j in 1:n) {
        a = y[x[, j] == 0]
        b = y[x[, j] == 1]
        if (length(a) >= 2 && length(b) >= 2) {
            test = t.test(a, b)
            p[j] = test$p.value
        } else {
            p[j] = 1
        }
    
        fdr = p.adjust(c(p, runif(N - n)), method='fdr')
        
        fdr_true_detected = sum(which(fdr < fdr_thr) <= n)
        
        fdr_detected = sum(fdr < fdr_thr)
    }
    
    c(minp=min(p),
      maxp=max(p),
      bonf_true=sum(p < alpha/N),
      fdr_detect=fdr_detected,
      fdr_true=fdr_true_detected,
      h2=var(yg)/var(y))
  }

```


```{r}
sim <-
  sim |>
  as_tibble() |>
  mutate(dataset = "C1: Sensitive TB-NM, n=407, h²=0.30")

sim_res <-
  sim_res |>
  as_tibble() |>
  mutate(dataset = "C2: S&R TB-NM, n=1000, h²=0.20")

sim_all <- rbind(sim, sim_res)
```

## Analysis
### No multiple test correction

```{r warning=FALSE}
sim_all |>
  ggplot(aes(minp,
             group = dataset,
             fill = dataset,
             alpha = 0.7)) +
  geom_histogram(bins=100,
                 colour = "grey40",
                 position = "identity") +
  guides(size = "legend", alpha = "none") +
  scale_x_log10(limits = c(1e-30, NA),
                breaks = c(1e-2, 1e-10, 1e-20, 1e-30)) +
  scale_fill_manual(values = c("bisque", "cyan")) +
  xlab("Minimal p-value") +
  ggtitle("Distribution of the uncorrected minimal p-value")
```

### Bonferroni

```{r}
sim_all |>
  as_tibble() |>
  ggplot(aes(bonf_true,
             colour = dataset,
             fill = dataset,
             alpha = 0.5)) +
  geom_histogram(binwidth=1,
                 colour = "grey40",
                 position = "identity") +
  guides(size = "legend", alpha = "none") +
  scale_x_continuous(breaks = seq(0, max(sim_all$bonf_true), 2)) +
  scale_fill_manual(values = c("bisque", "cyan")) +
  labs(seq(0, max(sim_all$bonf_true), 2)) +
  xlab("True associations") +
  ggtitle('Number of true associations detected using a Bonferroni correction')  
```

Here is a table of the minimal number of true associations that are likely to be found using Bonferroni:

```{r paged.print=FALSE}
sim_all |>
    as_tibble() |>
    group_by(dataset) |>
    count(bonf_true) |>
    arrange(desc(bonf_true)) |>
    mutate(cum_pc=signif(100 * cumsum(n/sum(n)),
                         digits=2)) |>
    arrange(dataset,
            desc(cum_pc),
            bonf_true) |>
  print(n = 20)
```
Note: The cum_pc column can be interpreted as the percentage of replicates having at least "bonf_true" true associations

### False Discovery Rate

How many hits, in total (true or false positives), would come out of the FDR analysis:

```{r}
sim_all |>
  as_tibble() |>
  ggplot(aes(fdr_detect,
             fill = dataset,
             alpha = 0.7)) +
  geom_histogram(binwidth=1,
                 colour = "grey40",
                  position="identity") +
  guides(size = "legend", alpha = "none") +
  scale_fill_manual(values = c("bisque", "cyan")) +
  xlab("Total number of hits") +
  ggtitle("Number of (true or false) hits using False Discovery Rate 50%")
```

How many true hits would come out of the FDR analysis:

```{r}
sim_all |>
  as_tibble() |>
  ggplot(aes(fdr_true,
             fill = dataset,
             alpha = 0.5)) +
  geom_histogram(binwidth=1,
                 colour = "grey40",
                 position = "identity") +
  guides(size = "legend", alpha = "none") +
  scale_fill_manual(values = c("bisque", "cyan")) +
  xlab("FDR true associations") +
  ggtitle("Number of true hits using False Discovery Rate 50%")
```

```{r paged.print=FALSE}
sim_all |>
  as_tibble() |>
  group_by(dataset) |>
  count(fdr_true) |>
  arrange(desc(fdr_true)) |>
  mutate(cum_pc=signif(100 * cumsum(n/sum(n)), digits=2)) |>
  arrange(dataset,
          desc(cum_pc),
          fdr_true) |>
  print(n = 50)
```
